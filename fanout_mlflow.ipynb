{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bc991b-092e-463d-9830-632d90ffc202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, os, time, pandas as pd, mlflow, joblib\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import functions as F\n",
    "import polars as pl\n",
    "from mlflow.exceptions import RestException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc125c2-ae29-470f-bf73-27014461aa11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MLF_EXPERIMENT = \"/Workspace/9900-f18a-cake/classifier\"\n",
    "\n",
    "client = MlflowClient()\n",
    "exp = client.get_experiment_by_name(MLF_EXPERIMENT)\n",
    "\n",
    "if exp is None:\n",
    "    try:\n",
    "        exp_id = client.create_experiment(MLF_EXPERIMENT)   # creates the Workspace experiment\n",
    "        print(\"Created experiment:\", MLF_EXPERIMENT, \"->\", exp_id)\n",
    "    except RestException as e:\n",
    "        # usually a permissions/folder issue under /Shared\n",
    "        print(\"Create failed:\", e)\n",
    "        raise\n",
    "else:\n",
    "    exp_id = exp.experiment_id\n",
    "    print(\"Found experiment:\", MLF_EXPERIMENT, \"->\", exp_id)\n",
    "\n",
    "mlflow.set_experiment(experiment_id=exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ec6721-6c77-44ea-8be9-d00eb17c4c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WORKER_NOTEBOOK = \"/Workspace/9900-f18a-cake/mt-method2/Training_model_MLFlow\"\n",
    "REPO_SRC   = \"/Workspace/9900-f18a-cake/mt-method2/src\"\n",
    "JOBLIB_PATH= \"/Workspace/9900-f18a-cake/mt-method2/data/freeze0525/diseaseTree_mapped.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca1de8d7-9adc-461f-9df1-ed8979bff1aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE_JOBS_API   = False      # False = sequential (works now); True = parallel via Jobs API\n",
    "MAX_IN_FLIGHT  = 12         # only used if USE_JOBS_API=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38391886-da1d-4403-b0d8-ebfa209dea69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FEATURES_TABLE = \"ccia.curated.filter_meth_mvalues_masked\"\n",
    "LABELS_TABLE   = \"ccia.curated.node_direct_labels\"     # your table with (node_id, sample_id, direct_label)\n",
    "LABEL_COL      = \"direct_label\"\n",
    "ID_COL         = \"sample_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c07e9f-c10c-415e-844e-5be61a31fb41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(REPO_SRC)\n",
    "tree = joblib.load(JOBLIB_PATH)\n",
    "root = getattr(tree, \"root\", tree)\n",
    "gname = lambda n: getattr(n, \"name\", getattr(n, \"label\", \"UNKNOWN\"))\n",
    "gkids = lambda n: getattr(n, \"children\", []) or getattr(n, \"child_nodes\", []) or []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24400590-25f0-4e56-955d-8ad1092a7216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_trainable(n, out=None):\n",
    "    out = out or []\n",
    "    ks = gkids(n)\n",
    "    if len(ks) >= 2: out.append(gname(n))\n",
    "    for k in ks: list_trainable(k, out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ea0f60-fea8-4634-abab-6c2bfc3e14fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nodes = list_trainable(root)\n",
    "print(\"Trainable parents:\", len(nodes))\n",
    "display(pd.DataFrame({\"node\": nodes}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10234c58-6be8-4d49-b995-5d609af8f8f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(MLF_EXPERIMENT)\n",
    "client = MlflowClient()\n",
    "\n",
    "extra_args = {\n",
    "    \"mlflow_experiment\": MLF_EXPERIMENT,\n",
    "    \"features_table\": FEATURES_TABLE,\n",
    "    \"labels_table\": LABELS_TABLE,\n",
    "    \"label_col\": LABEL_COL,\n",
    "    \"id_col\": ID_COL,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a23bb04-4fc3-468e-84f1-f78a4ced239a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"methyl_fanout_session\") as parent:\n",
    "    parent_id = parent.info.run_id\n",
    "    mlflow.set_tag(\"purpose\", \"hierarchical_train\")\n",
    "    mlflow.log_dict({\"nodes\": nodes, \"count\": len(nodes)}, \"plan/manifest.json\")\n",
    "\n",
    "    # default: sequential (no Jobs API; works without tokens)\n",
    "    if not USE_JOBS_API:\n",
    "        for n in nodes:\n",
    "            print(f\"[SEQUENTIAL] training {n}\")\n",
    "            dbutils.notebook.run(\n",
    "                WORKER_NOTEBOOK,\n",
    "                timeout_seconds=0,\n",
    "                arguments={\"only_node\": n, \"parent_run_id\": parent_id, **extra_args},\n",
    "            )\n",
    "        print(\"All nodes done (sequential).\")\n",
    "    else:\n",
    "        # ---- Parallel via Jobs API on the existing cluster (requires 'Run as' user/SP) ----\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        from databricks.sdk.service import jobs\n",
    "\n",
    "        host  = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "        token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "        w = WorkspaceClient(host=f\"https://{host}\", token=token)  # will work when this notebook is run AS user/SP\n",
    "\n",
    "        EXISTING_CLUSTER_ID = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "        assert EXISTING_CLUSTER_ID, \"Attach to a cluster first.\"\n",
    "\n",
    "        def submit(node_id: str) -> int:\n",
    "            payload = {\n",
    "                \"run_name\": f\"train_{node_id}\",\n",
    "                \"existing_cluster_id\": EXISTING_CLUSTER_ID,\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": WORKER_NOTEBOOK,\n",
    "                    \"base_parameters\": {\n",
    "                        \"only_node\": node_id,\n",
    "                        \"parent_run_id\": parent_id,\n",
    "                        **extra_args\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "            resp = w.api_client.do(\"POST\", \"/api/2.1/jobs/runs/submit\", body=payload)\n",
    "            return resp[\"run_id\"]\n",
    "\n",
    "        def finished(rid: int) -> bool:\n",
    "            s = w.runs.get(run_id=rid)\n",
    "            return s.state.life_cycle_state in {\"TERMINATED\", \"SKIPPED\", \"INTERNAL_ERROR\"}\n",
    "\n",
    "        def result_state(rid: int) -> str:\n",
    "            return (w.runs.get(run_id=rid).state.result_state) or \"UNKNOWN\"\n",
    "\n",
    "        pending = list(nodes); active = {}; fails = []\n",
    "        while pending and len(active) < MAX_IN_FLIGHT:\n",
    "            rid = submit(pending.pop(0)); active[rid] = rid\n",
    "\n",
    "        while active or pending:\n",
    "            done = []\n",
    "            for rid in list(active):\n",
    "                if finished(rid):\n",
    "                    rs = result_state(rid)\n",
    "                    print(f\"completed {rid} => {rs}\")\n",
    "                    if rs != \"SUCCESS\": fails.append((rid, rs))\n",
    "                    done.append(rid)\n",
    "            for rid in done: active.pop(rid, None)\n",
    "            while pending and len(active) < MAX_IN_FLIGHT:\n",
    "                rid = submit(pending.pop(0)); active[rid] = rid\n",
    "            if active: time.sleep(8)\n",
    "\n",
    "        if fails:\n",
    "            print(\"Failures:\", fails)\n",
    "        else:\n",
    "            print(\"All nodes done (parallel).\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fanout_mlflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
