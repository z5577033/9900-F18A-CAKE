{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8571470d-f7d0-46d2-bbf0-a12287ec4323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "\n",
    "print(\"Thread limits set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c19acc-3bac-4902-81ee-41fe99194427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install sqlmodel\n",
    "\n",
    "\n",
    "print(\"Thread limits set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff2bcf6-19fd-4950-8764-b64022f87cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raise_on_error = True   # --raise-on-error    \n",
    "disable_dm = True     #  --disable-dm     \n",
    "rf_n_jobs = 1           #  --rf-n-jobs 1   \n",
    "cv_n_jobs = 1           #  --cv-n-jobs 1     \n",
    "only_node = \"Haematological malignancy\"        #  --only-node \"Haematological malignancy\"   \n",
    "prefilter_topk = 200    #  --prefilter-topk 50 \n",
    "prefilter_scan_max = 5000   # --prefilter-scan-max 3000 \n",
    "prefilter_chunk_size = 1000     # --prefilter-chunk-size 500\n",
    "\n",
    "if disable_dm:\n",
    "    os.environ[\"MCH_DISABLE_DM\"] = \"1\"\n",
    "\n",
    "os.environ[\"RF_N_JOBS\"] = str(rf_n_jobs)\n",
    "os.environ[\"CV_N_JOBS\"] = str(cv_n_jobs)\n",
    "os.environ[\"MCH_PREFILTER_TOPK\"] = str(prefilter_topk)\n",
    "os.environ[\"MCH_PREFILTER_SCAN_MAX\"] = str(prefilter_scan_max)\n",
    "os.environ[\"MCH_PREFILTER_CHUNK_SIZE\"] = str(prefilter_chunk_size)\n",
    "if only_node:\n",
    "    os.environ[\"MCH_ONLY_NODE\"] = only_node\n",
    "\n",
    "print(\"Environment variables set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8b728a2-5d80-44ec-9a01-081d4ae1f5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "try:\n",
    "    test = pl.read_csv(\n",
    "        \"/Volumes/cb_prod/comp9300-9900-f18a-cake/9900-f18a-cake/data/mvalue_outputs_masked_subset_leukaemia_subsampled/MValue_polaris_pivot_0.csv\"\n",
    "    )\n",
    "    print(f\"Test CSV read. Shape: {test.height} rows, {test.width} columns\")\n",
    "    print(test.head(2))\n",
    "except Exception as e:\n",
    "    print(f\"Initial data check failed (this is expected if you are using the old path): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46efc14b-695e-4fee-97c8-bd646f452500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib.util\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Workaround: Attempt to clean up potential conflicting table definitions before loading ---\n",
    "\n",
    "try:\n",
    "    # Assume the conflicting table definition comes from mch.db.database_tables\n",
    "    # Try to remove it from Python's module cache\n",
    "    if 'mch.db.database_tables' in sys.modules:\n",
    "        del sys.modules['mch.db.database_tables']\n",
    "        print(\"üí° Module 'mch.db.database_tables' has been removed from the cache.\")\n",
    "        \n",
    "    # If you can determine where the AnalysisSet table object itself is stored,\n",
    "    # and that object has a MetaData attribute, you can try to clear it:\n",
    "    # from mch.db.database_tables import Base # Assume the base class is Base\n",
    "    # if hasattr(Base.metadata, 'tables') and 'zcc_analysis_set' in Base.metadata.tables:\n",
    "    #     del Base.metadata.tables['zcc_analysis_set']\n",
    "    #     print(\"üí° Conflicting table definition has been removed from MetaData.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error occurred while attempting to clean up conflicts: {e}\")\n",
    "    pass # Ignore cleanup failure and continue trying to load dynamically\n",
    "\n",
    "# --- Dynamic module loading (unchanged) ---\n",
    "SOURCE_PATH = r\"/Workspace/9900-f18a-cake/mt-method2/src/mch/models/train_logreg.py\"\n",
    "MODULE_NAME = \"train_logreg_module\" \n",
    "\n",
    "try:\n",
    "    if not os.path.exists(SOURCE_PATH):\n",
    "        raise FileNotFoundError(f\"Source file not found at: {SOURCE_PATH}\")\n",
    "    \n",
    "    spec = importlib.util.spec_from_file_location(MODULE_NAME, SOURCE_PATH)\n",
    "    custom_module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[MODULE_NAME] = custom_module \n",
    "    spec.loader.exec_module(custom_module)\n",
    "    \n",
    "    BatchModelTrainer = custom_module.BatchModelTrainer\n",
    "    print(f\"‚úÖ Successfully loaded module from absolute path: {SOURCE_PATH}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to dynamically load module: {e}\")\n",
    "    raise\n",
    "# ----------------------------------------------\n",
    "\n",
    "\n",
    "print(\"Starting training with Logistic Regression...\")\n",
    "trainer = BatchModelTrainer()\n",
    "stats = trainer.train_all_models(raise_on_error=raise_on_error)\n",
    "\n",
    "print(\"Training finished!\")\n",
    "display(stats)\n",
    "\n",
    "# --- Verify if the model is loaded correctly ---\n",
    "trained_model = trainer.models.get('Haematological malignancy')\n",
    "if trained_model and hasattr(trained_model, 'named_steps'):\n",
    "    model_type = trained_model.named_steps['modelGeneration']\n",
    "    print(f\"Model Type Actually Trained: {type(model_type)}\")\n",
    "# ------------------------------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Training_LogReg",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
