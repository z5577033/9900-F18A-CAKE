{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90759e9c-b1b5-4ce7-b4c9-d99f9b7ce59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import polars as pl\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(r\"/Workspace/9900-f18a-cake/working_branch/src\")\n",
    "\n",
    "\n",
    "CSV_PATH = \"/Volumes/cb_prod/comp9300-9900-f18a-cake/9900-f18a-cake/data/mvalue_outputs_masked/MValue_concat.csv\"\n",
    "# CSV_PATH = \"/Volumes/cb_prod/comp9300-9900-f18a-cake/9900-f18a-cake/data/mvalue_outputs_masked_subset_leukaemia_subsampled/MValue_polaris_pivot_0.csv\"\n",
    "# CSV_PATH = \"/Volumes/cb_prod/comp9300-9900-f18b-cake/9900-f18b-cake/data/mvalue_outputs_masked/MValue_concat.csv\"\n",
    "JOBLIB_PATH = \"/Workspace/9900-f18a-cake/working_branch/data/freeze0525/diseaseTree_mapped.joblib\"\n",
    "OUTPUT_PATH = \"/Workspace/9900-f18a-cake/working_branch/data/freeze0525/biosample_alignment.csv\"\n",
    "\n",
    "ID_PATTERN = re.compile(r\".*_T_.*_M$\")  # 2J0D2U4J_T_XJAFP6HU_M\n",
    "\n",
    "try:\n",
    "    df_csv = pl.read_csv(CSV_PATH, columns=[\"biosample_id\"], ignore_errors=True)\n",
    "except Exception:\n",
    "    df_csv = pl.read_csv(CSV_PATH, ignore_errors=True)\n",
    "    if \"biosample_id\" not in df_csv.columns:\n",
    "        for c in df_csv.columns:\n",
    "            if c.lower() == \"biosample_id\":\n",
    "                df_csv = df_csv.rename({c: \"biosample_id\"})\n",
    "                break\n",
    "\n",
    "csv_ids = [str(x).strip() for x in df_csv[\"biosample_id\"].to_list() if x and str(x).strip()]\n",
    "print(f\"read csv done {len(csv_ids)} samples total\")\n",
    "print(csv_ids[:10])\n",
    "\n",
    "def deep_scan_ids(obj, ids: set, _depth=0, _maxdepth=8):\n",
    "    if obj is None or _depth > _maxdepth:\n",
    "        return\n",
    "    if isinstance(obj, str):\n",
    "        if ID_PATTERN.match(obj):\n",
    "            ids.add(obj)\n",
    "        return\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        for it in obj:\n",
    "            deep_scan_ids(it, ids, _depth + 1)\n",
    "        return\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            if isinstance(k, str) and ID_PATTERN.match(k):\n",
    "                ids.add(k)\n",
    "            deep_scan_ids(v, ids, _depth + 1)\n",
    "        return\n",
    "    if hasattr(obj, \"__dict__\"):\n",
    "        deep_scan_ids(vars(obj), ids, _depth + 1)\n",
    "\n",
    "obj = joblib.load(JOBLIB_PATH)\n",
    "joblib_ids = set()\n",
    "deep_scan_ids(obj, joblib_ids)\n",
    "joblib_ids = sorted(joblib_ids)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "print(type(obj))\n",
    "pprint(obj)\n",
    "\n",
    "print(f\"read Joblib done {len(joblib_ids)} samples total\")\n",
    "print(joblib_ids[:10])\n",
    "\n",
    "\n",
    "csv_set = set(csv_ids)\n",
    "joblib_set = set(joblib_ids)\n",
    "\n",
    "intersection = csv_set & joblib_set\n",
    "csv_only = csv_set - joblib_set\n",
    "joblib_only = joblib_set - csv_set\n",
    "\n",
    "print(f\"ðŸ”¹ CSV count: {len(csv_set)}\")\n",
    "print(f\"ðŸ”¹ Joblib count: {len(joblib_set)}\")\n",
    "print(f\"ðŸ”¹ joint count: {len(intersection)}\")\n",
    "print(f\"ðŸ”¹ CSV only: {len(csv_only)}\")\n",
    "print(f\"ðŸ”¹ Joblib only: {len(joblib_only)}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.bar([\"CSV only\", \"Joblib only\", \"Align\"], \n",
    "        [len(csv_only), len(joblib_only), len(intersection)],\n",
    "        color=[\"#66c2a5\", \"#fc8d62\", \"#8da0cb\"])\n",
    "plt.ylabel(\"Sample cnt\")\n",
    "plt.title(\"Biosample ID joint\")\n",
    "for i, v in enumerate([len(csv_only), len(joblib_only), len(intersection)]):\n",
    "    plt.text(i, v + 2, str(v), ha=\"center\", fontweight=\"bold\")\n",
    "plt.show()\n",
    "\n",
    "alignment_df = pd.DataFrame({\n",
    "    \"biosample_id_csv\": list(csv_set),\n",
    "    \"in_joblib\": [x in joblib_set for x in csv_set]\n",
    "})\n",
    "\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "alignment_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Align output: {OUTPUT_PATH}\")\n",
    "alignment_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d67b4f3-d718-4453-8a1f-4c053c3db0b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import joblib\n",
    "import polars as pl\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(r\"/Workspace/9900-f18a-cake/working_branch/src\")\n",
    "\n",
    "obj = joblib.load(JOBLIB_PATH)\n",
    "\n",
    "print(type(obj))\n",
    "pprint(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bda6bc-bfb2-43d9-8460-6a643476d13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "REPO_SRC   = \"/Workspace/9900-f18a-cake/mt-method2/src\" \n",
    "JOBLIB_PATH= \"/Workspace/9900-f18a-cake/mt-method2/data/freeze0525/diseaseTree_mapped.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9596f39a-4c9f-4298-8c47-bbdda88a17a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, joblib, inspect, types\n",
    "\n",
    "if REPO_SRC not in sys.path:\n",
    "    sys.path.append(REPO_SRC)\n",
    "\n",
    "tree = joblib.load(JOBLIB_PATH)\n",
    "root = getattr(tree, \"root\", tree)\n",
    "\n",
    "print(\"Loaded:\", type(tree))\n",
    "print(\"Root:\", getattr(root, \"name\", getattr(root, \"label\", \"<unknown>\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b04d63-622f-465d-b7d5-1f4c4555a751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def node_name(n):\n",
    "    return getattr(n, \"name\", getattr(n, \"label\", \"UNKNOWN\"))\n",
    "\n",
    "def node_children(n):\n",
    "    # support a few common attribute names\n",
    "    for attr in [\"children\", \"child_nodes\", \"kids\", \"subnodes\"]:\n",
    "        v = getattr(n, attr, None)\n",
    "        if v: return list(v)\n",
    "    return []\n",
    "\n",
    "def node_child_names(n):\n",
    "    return {node_name(c) for c in node_children(n)}\n",
    "\n",
    "def walk_nodes(root):\n",
    "    q = deque([root])\n",
    "    seen = set()\n",
    "    while q:\n",
    "        n = q.popleft()\n",
    "        if id(n) in seen: \n",
    "            continue\n",
    "        seen.add(id(n))\n",
    "        yield n\n",
    "        q.extend(node_children(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76187811-73ca-433e-82a5-3c3c35f3b674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "LIKELY_LABEL_ATTRS = [\n",
    "    \"labels\",\"label_map\",\"sample_labels\",\"labels_by_sample\",\"y\",\"targets\",\"classes_by_sample\"\n",
    "]\n",
    "LIKELY_CLASS_LISTS = [\"classes\",\"class_names\",\"child_names\",\"labels_list\"]\n",
    "\n",
    "def try_extract_labels_from_node(n):\n",
    "    \"\"\"\n",
    "    Returns list[(biosample_id, node_id, direct_label)] or [] if not found.\n",
    "    \"\"\"\n",
    "    parent = node_name(n)\n",
    "    child_names = node_child_names(n)\n",
    "    out = []\n",
    "\n",
    "    # quick helpers\n",
    "    def looks_like_id(k):\n",
    "        return isinstance(k, str) and len(k) >= 6  # biosample ids are strings like NLTJ7CGJ..., adjust if needed\n",
    "\n",
    "    def commit_map(m):\n",
    "        for k, v in m.items():\n",
    "            if looks_like_id(k) and isinstance(v, str):\n",
    "                out.append((k, parent, v))\n",
    "        return out\n",
    "\n",
    "    g = vars(n).copy()  # node's attribute dict\n",
    "\n",
    "    # 1) direct dict attribute whose values are known child names\n",
    "    for k, v in g.items():\n",
    "        if isinstance(v, dict):\n",
    "            vals = list(v.values())\n",
    "            if vals and all(isinstance(x, str) for x in vals):\n",
    "                # if these labels are among this parent's children, accept\n",
    "                if child_names and all((x in child_names) for x in vals[: min(50, len(vals))]):\n",
    "                    return commit_map(v)\n",
    "\n",
    "    # 2) numeric class ids + a classes list we can map\n",
    "    classes_list = None\n",
    "    for key in LIKELY_CLASS_LISTS:\n",
    "        cl = g.get(key, None)\n",
    "        if isinstance(cl, (list, tuple)) and all(isinstance(x, str) for x in cl):\n",
    "            classes_list = list(cl)\n",
    "            break\n",
    "\n",
    "    for k, v in g.items():\n",
    "        if isinstance(v, dict) and classes_list and all(isinstance(x, int) for x in list(v.values())[:min(50, len(v))]):\n",
    "            # map ints to class strings if in range\n",
    "            mapped = {}\n",
    "            ok = True\n",
    "            for sid, cid in v.items():\n",
    "                if not looks_like_id(sid) or not (0 <= cid < len(classes_list)):\n",
    "                    ok = False\n",
    "                    break\n",
    "                mapped[sid] = classes_list[cid]\n",
    "            if ok:\n",
    "                return commit_map(mapped)\n",
    "\n",
    "    # 3) attributes named like labels that are dicts\n",
    "    for key in LIKELY_LABEL_ATTRS:\n",
    "        v = g.get(key, None)\n",
    "        if isinstance(v, dict):\n",
    "            vals = list(v.values())\n",
    "            if vals and all(isinstance(x, str) for x in vals):\n",
    "                # if no child_names, still accept (we'll trust the strings)\n",
    "                return commit_map(v)\n",
    "\n",
    "    # 4) pairs of arrays (sample_ids, labels)\n",
    "    # try to detect two arrays of same length; one stringy ids, the other strings that match children\n",
    "    arrays = {k: v for k, v in g.items() if isinstance(v, (list, tuple))}\n",
    "    for k1, a1 in arrays.items():\n",
    "        for k2, a2 in arrays.items():\n",
    "            if k1 == k2: \n",
    "                continue\n",
    "            if len(a1) == len(a2) and len(a1) > 0:\n",
    "                # id-array + label-array\n",
    "                if all(isinstance(x, str) for x in a1[:min(50,len(a1))]) and \\\n",
    "                   all(isinstance(x, str) for x in a2[:min(50,len(a2))]):\n",
    "                    # use them as (ids, labels)\n",
    "                    for sid, lab in zip(a1, a2):\n",
    "                        if looks_like_id(sid):\n",
    "                            out.append((sid, parent, lab))\n",
    "                    if out:\n",
    "                        return out\n",
    "\n",
    "    return out  # may be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c576496b-26e1-4af0-9135-e5125a051149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "per_node_counts = []\n",
    "\n",
    "for n in walk_nodes(root):\n",
    "    rows = try_extract_labels_from_node(n)\n",
    "    if rows:\n",
    "        all_rows.extend(rows)\n",
    "        per_node_counts.append((node_name(n), len(rows)))\n",
    "\n",
    "# Deduplicate (same sample might appear more than once per node)\n",
    "all_rows = list({(s,p,c) for (s,p,c) in all_rows})\n",
    "\n",
    "print(f\"Found rows: {len(all_rows)}\")\n",
    "display(spark.createDataFrame(per_node_counts, schema=\"node string, n int\").orderBy(F.desc(\"n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30179f98-3df8-4d83-bb99-cc2a0f8eb88b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n0 = root\n",
    "n1 = node_children(root)[0] if node_children(root) else None\n",
    "\n",
    "def dump_attrs(n, max_vals=5):\n",
    "    print(\"Node:\", node_name(n))\n",
    "    for k, v in vars(n).items():\n",
    "        t = type(v).__name__\n",
    "        # preview small containers\n",
    "        if isinstance(v, dict):\n",
    "            items = list(v.items())[:max_vals]\n",
    "            print(f\"  {k}: dict[{len(v)}] sample={items}\")\n",
    "        elif isinstance(v, (list, tuple)):\n",
    "            print(f\"  {k}: {t}[{len(v)}] sample={v[:max_vals]}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {t} -> {str(v)[:120]}\")\n",
    "\n",
    "dump_attrs(n0)\n",
    "if n1: \n",
    "    dump_attrs(n1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_check",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
